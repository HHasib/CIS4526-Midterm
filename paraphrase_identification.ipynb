{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "65a373c7",
   "metadata": {},
   "source": [
    "Initially, we import alll the necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 503,
   "id": "7f2361f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import svm\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from random import shuffle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19734ea0",
   "metadata": {},
   "source": [
    "We import the necessary files and process them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 504,
   "id": "38834b20",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "b'Skipping line 323: expected 4 fields, saw 5\\nSkipping line 823: expected 4 fields, saw 5\\nSkipping line 909: expected 4 fields, saw 5\\nSkipping line 1013: expected 4 fields, saw 5\\nSkipping line 1084: expected 4 fields, saw 5\\nSkipping line 1097: expected 4 fields, saw 5\\nSkipping line 1141: expected 4 fields, saw 5\\nSkipping line 1267: expected 4 fields, saw 5\\nSkipping line 1418: expected 4 fields, saw 5\\nSkipping line 1486: expected 4 fields, saw 5\\nSkipping line 1557: expected 4 fields, saw 5\\nSkipping line 1675: expected 4 fields, saw 5\\nSkipping line 1747: expected 4 fields, saw 5\\nSkipping line 1801: expected 4 fields, saw 5\\nSkipping line 1870: expected 4 fields, saw 5\\nSkipping line 2025: expected 4 fields, saw 5\\nSkipping line 2102: expected 4 fields, saw 5\\nSkipping line 2371: expected 4 fields, saw 5\\nSkipping line 2799: expected 4 fields, saw 5\\nSkipping line 3016: expected 4 fields, saw 5\\nSkipping line 3104: expected 4 fields, saw 5\\nSkipping line 3317: expected 4 fields, saw 5\\nSkipping line 3387: expected 4 fields, saw 5\\nSkipping line 3470: expected 4 fields, saw 5\\nSkipping line 3651: expected 4 fields, saw 5\\nSkipping line 3835: expected 4 fields, saw 5\\n'\n",
      "b'Skipping line 79: expected 4 fields, saw 5\\nSkipping line 418: expected 4 fields, saw 5\\nSkipping line 607: expected 4 fields, saw 5\\n'\n",
      "b'Skipping line 170: expected 3 fields, saw 4\\nSkipping line 389: expected 3 fields, saw 4\\nSkipping line 631: expected 3 fields, saw 4\\nSkipping line 656: expected 3 fields, saw 4\\nSkipping line 695: expected 3 fields, saw 4\\nSkipping line 809: expected 3 fields, saw 4\\n'\n"
     ]
    }
   ],
   "source": [
    "df_train = pd.read_csv('train_with_label.txt',sep='\\t',header=None, error_bad_lines= False)\n",
    "df_test = pd.read_csv('dev_with_label.txt', sep='\\t',header=None, error_bad_lines= False)\n",
    "df_test_nolabel =pd.read_csv('test_without_label.txt', sep='\\t',header=None, error_bad_lines= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 505,
   "id": "b931c9d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#setting column names for test and training set\n",
    "df_train.columns = [\"instance_id\",\"sent1\",\"sent2\",\"gold_score\"]\n",
    "df_test.columns = [\"instance_id\",\"sent1\",\"sent2\",\"gold_score\"]\n",
    "df_test_nolabel.columns = [\"instance_id\",\"sent1\",\"sent2\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 506,
   "id": "48487b80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "instance_id    0\n",
      "sent1          0\n",
      "sent2          0\n",
      "gold_score     0\n",
      "dtype: int64\n",
      "instance_id    0\n",
      "sent1          0\n",
      "sent2          0\n",
      "gold_score     0\n",
      "dtype: int64\n",
      "instance_id    0\n",
      "sent1          0\n",
      "sent2          0\n",
      "dtype: int64\n",
      "(3850, 4)\n",
      "(699, 4)\n",
      "(946, 3)\n"
     ]
    }
   ],
   "source": [
    "#remove all rows with null values\n",
    "df_train = df_train.dropna()\n",
    "print(df_train.isnull().sum())\n",
    "df_test = df_test.dropna()\n",
    "print(df_test.isnull().sum())\n",
    "df_test_nolabel = df_test_nolabel.dropna()\n",
    "print(df_test_nolabel.isnull().sum())\n",
    "print(df_train.shape)\n",
    "print(df_test.shape)\n",
    "print(df_test_nolabel.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 507,
   "id": "0dbc61bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data target names: [1. 0.]\n",
      "Test data target names: [0. 1.]\n"
     ]
    }
   ],
   "source": [
    "#possible target names \n",
    "print(\"Train data target names: {}\".format(df_train[\"gold_score\"].unique()))\n",
    "print(\"Test data target names: {}\".format(df_test[\"gold_score\"].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 508,
   "id": "70945d71",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training dataset\n",
    "df_train['sent1']=df_train['sent1'].astype('string')\n",
    "df_train['sent2']=df_train['sent2'].astype('string')\n",
    "df_train['sent1'] = df_train['sent1'].str.lower()\n",
    "df_train['sent2'] = df_train['sent2'].str.lower()\n",
    "#Test dataset\n",
    "df_test['sent1']=df_test['sent1'].astype('string')\n",
    "df_test['sent2']=df_test['sent2'].astype('string')\n",
    "df_test['sent1'] = df_test['sent1'].str.lower()\n",
    "df_test['sent2'] = df_test['sent2'].str.lower()\n",
    "#Test with no label\n",
    "df_test_nolabel['sent1']=df_test_nolabel['sent1'].astype('string')\n",
    "df_test_nolabel['sent2']=df_test_nolabel['sent2'].astype('string')\n",
    "df_test_nolabel['sent1'] = df_test_nolabel['sent1'].str.lower()\n",
    "df_test_nolabel['sent2'] = df_test_nolabel['sent2'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 509,
   "id": "0af70a61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "instance_id     object\n",
      "sent1           string\n",
      "sent2           string\n",
      "gold_score     float64\n",
      "dtype: object\n",
      "instance_id     object\n",
      "sent1           string\n",
      "sent2           string\n",
      "gold_score     float64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(df_train.dtypes)\n",
    "print(df_test.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 510,
   "id": "8e10fa2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training\n",
    "df_train['sent1'] = df_train['sent1'].str.replace(\",\",\"\")\n",
    "df_train['sent2'] = df_train['sent2'].str.replace(\",\",\"\")\n",
    "#Testing\n",
    "df_test['sent1'] = df_test['sent1'].str.replace(\",\",\"\")\n",
    "df_test['sent2'] = df_test['sent2'].str.replace(\",\",\"\")\n",
    "#Testing with no label\n",
    "df_test_nolabel['sent1'] = df_test_nolabel['sent1'].str.replace(\",\",\"\")\n",
    "df_test_nolabel['sent2'] = df_test_nolabel['sent2'].str.replace(\",\",\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7404b57c",
   "metadata": {},
   "source": [
    "## Feature 1: WordCount difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 511,
   "id": "2d37e084",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training\n",
    "df_train['sent1l'] = df_train['sent1'].str.split()\n",
    "df_train['sent2l'] = df_train['sent2'].str.split()\n",
    "#Testing\n",
    "df_test['sent1l'] = df_test['sent1'].str.split()\n",
    "df_test['sent2l'] = df_test['sent2'].str.split()\n",
    "#Testing with no label\n",
    "df_test_nolabel['sent1l'] = df_test_nolabel['sent1'].str.split()\n",
    "df_test_nolabel['sent2l'] = df_test_nolabel['sent2'].str.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 512,
   "id": "4e96223d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training\n",
    "df_train['sent1l']=df_train['sent1l'].apply(lambda x: len(x))\n",
    "df_train['sent2l']=df_train['sent2l'].apply(lambda x: len(x))\n",
    "#Testing\n",
    "df_test['sent1l']=df_test['sent1l'].apply(lambda x: len(x))\n",
    "df_test['sent2l']=df_test['sent2l'].apply(lambda x: len(x))\n",
    "#Testing with no label\n",
    "df_test_nolabel['sent1l']=df_test_nolabel['sent1l'].apply(lambda x: len(x))\n",
    "df_test_nolabel['sent2l']=df_test_nolabel['sent2l'].apply(lambda x: len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 513,
   "id": "9db9a43b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_train.assign(wcd=lambda x: abs((x['sent1l']-x['sent2l'])))\n",
    "df_test = df_test.assign(wcd=lambda x: abs((x['sent1l']-x['sent2l'])))\n",
    "df_test_nolabel = df_test_nolabel.assign(wcd=lambda x: abs((x['sent1l']-x['sent2l'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 514,
   "id": "83ea6f2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "instance_id    0\n",
      "sent1          0\n",
      "sent2          0\n",
      "sent1l         0\n",
      "sent2l         0\n",
      "wcd            0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df_test_nolabel.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08e5556d",
   "metadata": {},
   "source": [
    "## Feature 2: Fuzzywuzzy ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 515,
   "id": "b84baaea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fuzzywuzzy import fuzz\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 516,
   "id": "d5327025",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['fuzz_ratio'] = df_train.apply(lambda row: fuzz.ratio(row['sent1'].translate(str.maketrans('', '', string.punctuation)), row['sent2'].translate(str.maketrans('', '', string.punctuation))), axis = 1)\n",
    "df_test['fuzz_ratio'] = df_test.apply(lambda row: fuzz.ratio(row['sent1'].translate(str.maketrans('', '', string.punctuation)), row['sent2'].translate(str.maketrans('', '', string.punctuation))), axis = 1)\n",
    "df_test_nolabel['fuzz_ratio'] = df_test_nolabel.apply(lambda row: fuzz.ratio(row['sent1'].translate(str.maketrans('', '', string.punctuation)), row['sent2'].translate(str.maketrans('', '', string.punctuation))), axis = 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 517,
   "id": "37947318",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['fuzz_token_sort_ratio'] = df_train.apply(lambda row: fuzz.token_sort_ratio(row['sent1'].translate(str.maketrans('', '', string.punctuation)), row['sent2'].translate(str.maketrans('', '', string.punctuation))), axis = 1)\n",
    "df_test['fuzz_token_sort_ratio'] = df_test.apply(lambda row: fuzz.token_sort_ratio(row['sent1'].translate(str.maketrans('', '', string.punctuation)), row['sent2'].translate(str.maketrans('', '', string.punctuation))), axis = 1)\n",
    "df_test_nolabel['fuzz_token_sort_ratio'] = df_test_nolabel.apply(lambda row: fuzz.token_sort_ratio(row['sent1'].translate(str.maketrans('', '', string.punctuation)), row['sent2'].translate(str.maketrans('', '', string.punctuation))), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 518,
   "id": "4f582d31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ed2a479",
   "metadata": {},
   "source": [
    "## Feature 3: Bleu score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 519,
   "id": "a6a3dcb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from nltk.translate.bleu_score import SmoothingFunction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 520,
   "id": "e17442f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['bleu_score'] = df_train.apply(lambda row: sentence_bleu(row['sent1'].translate(str.maketrans('', '', string.punctuation)), row['sent2'].translate(str.maketrans('', '', string.punctuation)),smoothing_function=SmoothingFunction().method4), axis = 1)\n",
    "df_test['bleu_score'] = df_test.apply(lambda row: sentence_bleu(row['sent1'].translate(str.maketrans('', '', string.punctuation)), row['sent2'].translate(str.maketrans('', '', string.punctuation)),smoothing_function=SmoothingFunction().method4),axis = 1)\n",
    "df_test_nolabel['bleu_score'] = df_test_nolabel.apply(lambda row: sentence_bleu(row['sent1'].translate(str.maketrans('', '', string.punctuation)), row['sent2'].translate(str.maketrans('', '', string.punctuation)),smoothing_function=SmoothingFunction().method4),axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 521,
   "id": "cd18e641",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_test_nolabel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e732b60b",
   "metadata": {},
   "source": [
    "## Reshaping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 522,
   "id": "564e0df8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "instance_id               object\n",
      "sent1                     string\n",
      "sent2                     string\n",
      "gold_score               float64\n",
      "sent1l                     int64\n",
      "sent2l                     int64\n",
      "wcd                        int64\n",
      "fuzz_ratio                 int64\n",
      "fuzz_token_sort_ratio      int64\n",
      "bleu_score               float64\n",
      "dtype: object\n",
      "instance_id               object\n",
      "sent1                     string\n",
      "sent2                     string\n",
      "gold_score               float64\n",
      "sent1l                     int64\n",
      "sent2l                     int64\n",
      "wcd                        int64\n",
      "fuzz_ratio                 int64\n",
      "fuzz_token_sort_ratio      int64\n",
      "bleu_score               float64\n",
      "dtype: object\n",
      "instance_id               object\n",
      "sent1                     string\n",
      "sent2                     string\n",
      "sent1l                     int64\n",
      "sent2l                     int64\n",
      "wcd                        int64\n",
      "fuzz_ratio                 int64\n",
      "fuzz_token_sort_ratio      int64\n",
      "bleu_score               float64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(df_train.dtypes)\n",
    "print(df_test.dtypes)\n",
    "print(df_test_nolabel.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 523,
   "id": "e146e362",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_val: (3850, 4), test: (699, 4), test_nolabel: (946, 4)\n"
     ]
    }
   ],
   "source": [
    "X_train = df_train.drop(columns=['instance_id','sent1','sent2','gold_score','sent1l','sent2l']).values\n",
    "X_test = df_test.drop(columns=['instance_id','sent1','sent2','gold_score','sent1l','sent2l']).values\n",
    "y_train = df_train['gold_score'].values\n",
    "y_test =df_test['gold_score'].values\n",
    "X_test_nolabel = df_test_nolabel.drop(columns=['instance_id','sent1','sent2','sent1l','sent2l']).values\n",
    "\n",
    "normalizer = StandardScaler()\n",
    "# X_train_val = X_train_val.reshape(-1,1)\n",
    "# X_test = X_test.reshape(-1,1)\n",
    "X_train = normalizer.fit_transform(X_train)\n",
    "X_test = normalizer.transform(X_test)\n",
    "X_test_nolabel = normalizer.transform(X_test_nolabel)\n",
    "print(\"train_val: {}, test: {}, test_nolabel: {}\".format(X_train.shape, X_test.shape, X_test_nolabel.shape))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc1c8cc6",
   "metadata": {},
   "source": [
    "## Logistic Regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 524,
   "id": "5ac45fb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.519, recall: 0.994, precision: 0.511, f1: 0.675,\n"
     ]
    }
   ],
   "source": [
    "clflr = LogisticRegression(penalty='l2', solver='lbfgs')\n",
    "clflr.fit(X_train_val, y_train_val)\n",
    "\n",
    "# evaluate the model on the testing set\n",
    "y_test_pred = clflr.predict(X_test)\n",
    "\n",
    "\n",
    "acc = accuracy_score(y_test, y_test_pred)\n",
    "f1 = f1_score(y_test, y_test_pred)\n",
    "recall = recall_score(y_test, y_test_pred)\n",
    "precision = precision_score(y_test, y_test_pred)\n",
    "\n",
    "\n",
    "print(\"accuracy: {:.3f}, recall: {:.3f}, precision: {:.3f}, f1: {:.3f},\".format(acc, recall, precision, f1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b3c5175",
   "metadata": {},
   "source": [
    "## SVM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 525,
   "id": "c566495e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.612, recall: 0.952, precision: 0.568, f1: 0.711,\n"
     ]
    }
   ],
   "source": [
    "clf = svm.SVC()\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# evaluate the model on the testing set\n",
    "y_test_pred = clf.predict(X_test)\n",
    "\n",
    "\n",
    "acc = accuracy_score(y_test, y_test_pred)\n",
    "f1 = f1_score(y_test, y_test_pred)\n",
    "recall = recall_score(y_test, y_test_pred)\n",
    "precision = precision_score(y_test, y_test_pred)\n",
    "\n",
    "\n",
    "print(\"accuracy: {:.3f}, recall: {:.3f}, precision: {:.3f}, f1: {:.3f},\".format(acc, recall, precision, f1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5a62284",
   "metadata": {},
   "source": [
    "## Prediction for test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 526,
   "id": "db11017e",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred = clf.predict(X_test_nolabel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 527,
   "id": "7377eb34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "946\n"
     ]
    }
   ],
   "source": [
    "iterations = df_test_nolabel['instance_id'].to_numpy()\n",
    "# print(dt)\n",
    "print(len(iterations))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 528,
   "id": "dc3b00f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "946\n"
     ]
    }
   ],
   "source": [
    "# print(y_test_pred)\n",
    "print(len(y_test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 529,
   "id": "1b632b5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open('AbuHasnatHasib_test_result.txt', 'w') #write to file\n",
    "\n",
    "for i in range(0,len(iterations)-1):\n",
    "    file.write(str(iterations[i]) + \"\\t\" + str(y_test_pred[i]) + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 530,
   "id": "8727beed",
   "metadata": {},
   "outputs": [],
   "source": [
    "file.close() #close file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "406f732b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "f43b03af1b6c7d2647a9c83c0faa395d808984b869aa737ec4f7298e40a0df4a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
